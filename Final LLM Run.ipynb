{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd5e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2-Handed model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Model and labels loaded. Starting camera...\n",
      "\n",
      "--- Awaiting new input ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHMAN SODHI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer updated: 'E'\n",
      "Buffer updated: 'EG'\n",
      "Buffer updated: 'EGM'\n",
      "Buffer updated: 'EGMF'\n",
      "Sending to LLM: 'EGMF' (Length: 4)\n",
      "Raw LLM response: 'MPEG'\n",
      "Cleaned output: 'MPEG'\n",
      "\n",
      "--- Awaiting new input ---\n",
      "\n",
      "Buffer updated: 'F'\n",
      "Buffer updated: 'FI'\n",
      "Buffer updated: 'FI7'\n",
      "Buffer updated: 'FI7Z'\n",
      "Buffer updated: 'FI7ZM'\n",
      "Buffer updated: 'FI7ZMA'\n",
      "Buffer updated: 'FI7ZMAY'\n",
      "Buffer updated: 'FI7ZMAYL'\n",
      "Sending to LLM: 'FI7ZMAYL' (Length: 8)\n",
      "Raw LLM response: 'FIZZY'\n",
      "Cleaned output: 'FIZZY'\n",
      "\n",
      "--- Awaiting new input ---\n",
      "\n",
      "Buffer updated: 'F'\n",
      "Buffer updated: 'FI'\n",
      "Buffer updated: 'FIM'\n",
      "Buffer updated: 'FIMA'\n",
      "Buffer updated: 'FIMAL'\n",
      "Sending to LLM: 'FIMAL' (Length: 5)\n",
      "Raw LLM response: 'FIMAL'\n",
      "Cleaned output: 'FIMAL'\n",
      "\n",
      "--- Awaiting new input ---\n",
      "\n",
      "Buffer updated: 'G'\n",
      "Buffer updated: 'GZ'\n",
      "Buffer updated: 'GZO'\n",
      "Buffer updated: 'GZOS'\n",
      "Buffer updated: 'GZOSY'\n",
      "Sending to LLM: 'GZOSY' (Length: 5)\n",
      "Raw LLM response: 'Glossy'\n",
      "Cleaned output: 'GLOSSY'\n",
      "\n",
      "--- Awaiting new input ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox # Import messagebox explicitly for the popup\n",
    "import re # Added for clean string manipulation\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = 'isl_model_2hand.h5'\n",
    "LABELS_PATH = 'labels_2hand.json'\n",
    "NUM_HANDS = 2\n",
    "FEATURES_PER_HAND = 42 # 21 landmarks * 2 coords\n",
    "TOTAL_FEATURES = NUM_HANDS * FEATURES_PER_HAND # 84\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"llama3.2:3b\" # CHANGE THIS if you use a different model\n",
    "\n",
    "# --- Buffer and Timing Configuration ---\n",
    "MIN_BUFFER_SIZE = 3\n",
    "TIMEOUT_SECONDS = 5.0\n",
    "SKIP_CHAR = 'Q' # Character to skip at start/end of buffer\n",
    "REQUIRED_STABILITY_FRAMES = 5 # Frames a character must be held to be accepted\n",
    "\n",
    "# --- Global State for Buffer and Timing ---\n",
    "char_buffer = \"\"\n",
    "last_char_time = time.time()\n",
    "camera_is_running = True # State variable for the camera loop\n",
    "\n",
    "# --- Global State for Stability Tracking ---\n",
    "current_stable_char = \"\"\n",
    "stability_counter = 0\n",
    "\n",
    "# --- Load Model and Labels ---\n",
    "print(\"Loading 2-Handed model...\")\n",
    "try:\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(f\"Please make sure '{MODEL_PATH}' is in the same folder.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Loading labels...\")\n",
    "try:\n",
    "    with open(LABELS_PATH, 'r') as f:\n",
    "        # Load and convert keys from string '0' to int 0\n",
    "        label_map_str = json.load(f)\n",
    "        label_map = {int(k): v for k, v in label_map_str.items()}\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{LABELS_PATH}' not found.\")\n",
    "    print(\"Please make sure it is in the same folder.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Model and labels loaded. Starting camera...\")\n",
    "\n",
    "# --- MediaPipe and OpenCV Setup ---\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=NUM_HANDS,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# --- Normalization Function (Must be IDENTICAL to training) ---\n",
    "def normalize_landmarks(landmarks_raw):\n",
    "    \"\"\"Normalizes hand landmarks relative to the wrist.\"\"\"\n",
    "    landmarks_rel = []\n",
    "    wrist_x, wrist_y = landmarks_raw[0]\n",
    "    for x, y in landmarks_raw:\n",
    "        landmarks_rel.append((x - wrist_x, y - wrist_y))\n",
    "    \n",
    "    max_val = max(abs(coord) for point in landmarks_rel for coord in point)\n",
    "    if max_val == 0:\n",
    "        return [0.0] * FEATURES_PER_HAND\n",
    "        \n",
    "    landmarks_norm = [(x / max_val, y / max_val) for x, y in landmarks_rel]\n",
    "    return np.array(landmarks_norm).flatten()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# --- Ollama and Tkinter Popup Functions (MODIFIED) ---\n",
    "# -----------------------------------------------\n",
    "\n",
    "def call_ollama(prompt_text, word_length):\n",
    "    \"\"\"\n",
    "    Calls Ollama with the specific required prompt.\n",
    "    \"\"\"\n",
    "    print(f\"Sending to LLM: '{prompt_text}' (Length: {word_length})\")\n",
    "\n",
    "    # --- REQUIRED LLM PROMPT ---\n",
    "    llm_prompt = (\n",
    "        f\"The user signed a sequence of {word_length} characters: '{prompt_text}'. \"\n",
    "        \"Assume that one or two of these characters might be misread due to signing errors \"\n",
    "        \"or model uncertainty. \"\n",
    "        f\"What is the single, most likely English word of exactly {word_length} letters \"\n",
    "        \"that the user was trying to spell? \"\n",
    "        \"Only return the predicted word, nothing else.\"\n",
    "    )\n",
    "    # ---------------------------\n",
    "\n",
    "    data = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": llm_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.3,\n",
    "            \"top_p\": 0.9,\n",
    "            \"num_predict\": word_length + 5, # Limit output length slightly above word length\n",
    "            \"stop\": [\"\\n\", \"Input:\", \"Output:\", \"Explanation:\", \"word\", \"word is\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=data, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        raw_result = response.json().get('response', '').strip()\n",
    "        \n",
    "        print(f\"Raw LLM response: '{raw_result}'\")\n",
    "        \n",
    "        # Aggressively clean the output to extract a single word/sequence\n",
    "        # 1. Take the first line\n",
    "        cleaned = raw_result.split('\\n')[0].strip()\n",
    "        # 2. Remove all non-alphabetic characters and convert to uppercase\n",
    "        cleaned = re.sub(r'[^A-Za-z]', '', cleaned).upper()\n",
    "        \n",
    "        if cleaned:\n",
    "            print(f\"Cleaned output: '{cleaned}'\")\n",
    "            return cleaned\n",
    "        else:\n",
    "            return f\"[Unable to parse] Raw: {raw_result}\"\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return f\"Error: Could not connect to Ollama at {OLLAMA_URL}. Is it running?\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: LLM request timed out\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def show_llm_popup(input_chars):\n",
    "    \"\"\"\n",
    "    Shows a popup with the detected characters and the LLM result.\n",
    "    Stops the camera while the popup is active.\n",
    "    \"\"\"\n",
    "    global char_buffer, camera_is_running, current_stable_char, stability_counter\n",
    "\n",
    "    # 1. STOP CAMERA\n",
    "    camera_is_running = False\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # 2. Prepare LLM parameters\n",
    "    word_length = len(input_chars)\n",
    "    \n",
    "    # 3. CALL LLM\n",
    "    llm_output = call_ollama(input_chars, word_length)\n",
    "\n",
    "    # 4. SHOW POPUP\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    result_message = (\n",
    "        f\"Detected Characters: {input_chars}\\n\\n\"\n",
    "        f\"LLM Interpretation:\\n\"\n",
    "        f\"'{llm_output}'\"\n",
    "    )\n",
    "    \n",
    "    messagebox.showinfo(\"Sign Language Interpretation Result\", result_message)\n",
    "\n",
    "    # 5. RESET STATE AND RESTART CAMERA\n",
    "    char_buffer = \"\"\n",
    "    current_stable_char = \"\"\n",
    "    stability_counter = 0\n",
    "    camera_is_running = True\n",
    "    print(\"\\n--- Awaiting new input ---\\n\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# --- Real-Time Loop ---\n",
    "# -----------------------------------------------\n",
    "\n",
    "print(\"\\n--- Awaiting new input ---\\n\")\n",
    "while cap.isOpened():\n",
    "    # Only run the main loop body if the camera is marked as running\n",
    "    if camera_is_running:\n",
    "        # Check if the video capture object is valid before reading\n",
    "        if not cap.isOpened():\n",
    "             print(\"Camera was closed unexpectedly. Attempting to reopen...\")\n",
    "             cap = cv2.VideoCapture(0)\n",
    "             if not cap.isOpened():\n",
    "                 print(\"Failed to reopen camera. Exiting.\")\n",
    "                 break\n",
    "             time.sleep(1) # Give camera time to initialize\n",
    "             continue\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # Still check the buffer timeout even if frame is empty\n",
    "            pass\n",
    "        else:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(img_rgb)\n",
    "            \n",
    "            all_hands_features = []\n",
    "            \n",
    "            # Draw landmarks and collect features\n",
    "            if results.multi_hand_landmarks:\n",
    "                num_hands_detected = len(results.multi_hand_landmarks)\n",
    "                \n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    \n",
    "                    landmarks_raw = [(lm.x, lm.y) for lm in hand_landmarks.landmark]\n",
    "                    landmarks_norm = normalize_landmarks(landmarks_raw)\n",
    "                    all_hands_features.extend(landmarks_norm)\n",
    "\n",
    "                if num_hands_detected < NUM_HANDS:\n",
    "                    padding = [0.0] * FEATURES_PER_HAND * (NUM_HANDS - num_hands_detected)\n",
    "                    all_hands_features.extend(padding)\n",
    "            else:\n",
    "                all_hands_features = [0.0] * TOTAL_FEATURES\n",
    "\n",
    "            # --- Prediction and Stability Logic ---\n",
    "            if len(all_hands_features) == TOTAL_FEATURES:\n",
    "                data_array = np.array([all_hands_features])\n",
    "                prediction = model.predict(data_array, verbose=0)\n",
    "                \n",
    "                class_index = np.argmax(prediction[0])\n",
    "                confidence = np.max(prediction[0]) * 100\n",
    "                \n",
    "                # Global access required for stability tracking and buffer\n",
    "                global current_stable_char, stability_counter, char_buffer, last_char_time\n",
    "                \n",
    "                # Check for the minimum required confidence\n",
    "                if confidence > 80: # Using the hardcoded 80 from original logic\n",
    "                    predicted_char = label_map[class_index]\n",
    "                    current_char = predicted_char\n",
    "                    text = f\"{predicted_char} ({confidence:.2f}%)\"\n",
    "\n",
    "                    # --- Stability Check ---\n",
    "                    if current_char == current_stable_char:\n",
    "                        stability_counter += 1\n",
    "                    else:\n",
    "                        current_stable_char = current_char\n",
    "                        stability_counter = 1 \n",
    "\n",
    "                    # --- Buffer Management (Triggered by Stability) ---\n",
    "                    if stability_counter >= REQUIRED_STABILITY_FRAMES:\n",
    "                        \n",
    "                        # Check if the stable character is different from the last one in the buffer\n",
    "                        is_new_char_for_buffer = not char_buffer or current_stable_char != char_buffer[-1]\n",
    "                        \n",
    "                        if is_new_char_for_buffer:\n",
    "                            \n",
    "                            # 1. Skip character 'Q' on the first frame\n",
    "                            if not char_buffer and current_stable_char == SKIP_CHAR:\n",
    "                                print(f\"Ignored first stable char: {current_stable_char}\")\n",
    "                                # Reset stability to keep waiting for a valid start\n",
    "                                current_stable_char = \"\"\n",
    "                                stability_counter = 0 \n",
    "                            \n",
    "                            # 2. Add stable, new character to the main buffer\n",
    "                            else:\n",
    "                                char_buffer += current_stable_char\n",
    "                                last_char_time = time.time() # Reset timer on new character\n",
    "                                print(f\"Buffer updated: '{char_buffer}'\")\n",
    "                                # Reset stability counter after acceptance\n",
    "                                stability_counter = 0 \n",
    "                                current_stable_char = \"\" # Clear tracking char to look for the next distinct sign\n",
    "                                \n",
    "                else:\n",
    "                    text = \"...\"\n",
    "                    # If confidence is low, reset stability tracking completely\n",
    "                    stability_counter = 0\n",
    "                    current_stable_char = \"\"\n",
    "                    \n",
    "                # Display the result\n",
    "                (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "                cv2.rectangle(frame, (10, 10), (10 + text_width + 10, 10 + text_height + baseline + 10), (0,0,0), -1)\n",
    "                cv2.putText(frame, text, (20, 20 + text_height), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display the current buffer and stability count\n",
    "            buffer_text = f\"Buffer: {char_buffer}\"\n",
    "            stability_text = f\"Frames: {stability_counter}/{REQUIRED_STABILITY_FRAMES}\"\n",
    "            \n",
    "            # Draw Buffer\n",
    "            cv2.putText(frame, buffer_text, (frame.shape[1] - 300, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            # Draw Stability Status\n",
    "            cv2.putText(frame, stability_text, (frame.shape[1] - 300, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Show the frame\n",
    "            cv2.imshow('ISL Landmark Detection (2-Handed)', frame)\n",
    "    \n",
    "    # --- Timeout Check (Runs even if frame is empty or camera is paused) ---\n",
    "    current_time = time.time()\n",
    "    if char_buffer and (current_time - last_char_time) > TIMEOUT_SECONDS and len(char_buffer) >= MIN_BUFFER_SIZE:\n",
    "        \n",
    "        final_chars = char_buffer\n",
    "        \n",
    "        # 3. Skip 'Q' on the last character (if it's the last one)\n",
    "        if final_chars[-1] == SKIP_CHAR:\n",
    "            print(f\"Removed final character '{SKIP_CHAR}' before processing.\")\n",
    "            final_chars = final_chars[:-1]\n",
    "            \n",
    "            # Check if the buffer is still long enough after removing 'Q'\n",
    "            if len(final_chars) < MIN_BUFFER_SIZE:\n",
    "                 print(f\"Buffer too short ({len(final_chars)}) after removing 'Q'. Keeping the shortened buffer and resetting timer.\")\n",
    "                 char_buffer = final_chars # Update buffer to exclude the final 'Q'\n",
    "                 last_char_time = time.time() # Reset timer to wait for new input\n",
    "                 continue # Skip popup this cycle and wait for new input\n",
    "\n",
    "        # Perform LLM call and popup, which resets state and restarts the camera\n",
    "        show_llm_popup(final_chars)\n",
    "        \n",
    "    # Handle user quit command\n",
    "    if cv2.waitKey(5) & 0xFF == 27: # Press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47028c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
